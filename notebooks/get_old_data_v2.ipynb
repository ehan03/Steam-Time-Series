{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9325e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from json import loads\n",
    "from functools import reduce\n",
    "from waybackpy import WaybackMachineCDXServerAPI\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf62a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRAPE_URLS = False\n",
    "SCRAPE_JSONP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRAPE_URLS:\n",
    "    if not os.path.exists(\"snapshot_urls.txt\"):\n",
    "        url = \"https://store.steampowered.com/stats/content/\"\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36\"\n",
    "        cdx = WaybackMachineCDXServerAPI(url, user_agent, start_timestamp=\"20080101000000\", end_timestamp=\"20270101000000\")\n",
    "        snapshot_urls = []\n",
    "        for item in cdx.snapshots():\n",
    "            snapshot_urls.append(item.archive_url)\n",
    "\n",
    "        with open(\"snapshot_urls.txt\", \"w\") as f:\n",
    "            for url in snapshot_urls:\n",
    "                f.write(url + \"\\n\")\n",
    "    \n",
    "    # Extract JSONP URLs from snapshot pages\n",
    "    bad_urls = []\n",
    "    if not os.path.exists(\"jsonp_urls.txt\"):\n",
    "        with open(\"snapshot_urls.txt\", \"r\") as f:\n",
    "            snapshot_urls = f.read().splitlines()\n",
    "\n",
    "        jsonp_urls = []\n",
    "        for url in tqdm(snapshot_urls, desc=\"Processing snapshot URLs\"):\n",
    "            try:\n",
    "                ua = UserAgent()\n",
    "                headers = {\"User-Agent\": ua.random}\n",
    "                response = requests.get(url, headers=headers, timeout=300)\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error fetching URL {url}: {e}\")\n",
    "                bad_urls.append(url)\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            for script in soup.find_all(\"script\"):\n",
    "                if script.string and \"contentserver_bandwidth_stacked.jsonp\" in script.string:\n",
    "                    match = re.search(r'\"(https?://[^\"]*contentserver_bandwidth_stacked\\.jsonp[^\"]*)\"', script.string)\n",
    "                    if match:\n",
    "                        jsonp_urls.append(match.group(1))\n",
    "            time.sleep(10)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "        with open(\"jsonp_urls.txt\", \"w\") as f:\n",
    "            for url in jsonp_urls:\n",
    "                f.write(url + \"\\n\")\n",
    "        \n",
    "        if bad_urls:\n",
    "            with open(\"bad_urls.txt\", \"w\") as f:\n",
    "                for url in bad_urls:\n",
    "                    f.write(url + \"\\n\")\n",
    "        \n",
    "    # Reprocess bad URLs\n",
    "    with open(\"bad_urls.txt\", \"r\") as f:\n",
    "        bad_urls = f.read().splitlines()\n",
    "\n",
    "    jsonp_urls = []\n",
    "    for url in tqdm(bad_urls, desc=\"Reprocessing bad URLs\"):\n",
    "        try:\n",
    "            ua = UserAgent()\n",
    "            headers = {\"User-Agent\": ua.random}\n",
    "            response = requests.get(url, headers=headers, timeout=300)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL {url}: {e}\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        for script in soup.find_all(\"script\"):\n",
    "            if script.string and \"contentserver_bandwidth_stacked.jsonp\" in script.string:\n",
    "                match = re.search(r'\"(https?://[^\"]*contentserver_bandwidth_stacked\\.jsonp[^\"]*)\"', script.string)\n",
    "                if match:\n",
    "                    jsonp_urls.append(match.group(1))\n",
    "        time.sleep(10)  # Sleep to avoid overwhelming the server\n",
    "\n",
    "    with open(\"jsonp_urls.txt\", \"a\") as f:\n",
    "        for url in jsonp_urls:\n",
    "            f.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf38153",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCRAPE_JSONP:\n",
    "    with open(\"jsonp_urls.txt\", \"r\") as f:\n",
    "        urls = f.read().splitlines()\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    bad_urls = []\n",
    "    for url in tqdm(urls):\n",
    "        try:\n",
    "            ua = UserAgent()\n",
    "            headers = {\"User-Agent\": ua.random}\n",
    "            response = requests.get(url, headers=headers, timeout=300)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL {url}: {e}\")\n",
    "            bad_urls.append(url)\n",
    "            continue\n",
    "        startidx = response.text.find(\"(\")\n",
    "        endidx = response.text.find(\")\")\n",
    "\n",
    "        try:\n",
    "            data = loads(response.text[startidx + 1 : endidx])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSONP from URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if \"json\" not in data:\n",
    "            continue\n",
    "\n",
    "        series_list = loads(data[\"json\"])\n",
    "\n",
    "        df_list = []\n",
    "        for series in series_list:\n",
    "            df_dict = {}\n",
    "            region = series[\"label\"]\n",
    "            df_dict[\"Timestamp\"] = [\n",
    "                pd.to_datetime(x[0], unit=\"ms\") for x in series[\"data\"]\n",
    "            ]\n",
    "            df_dict[region] = [int(x[1]) for x in series[\"data\"]]\n",
    "            df_list.append(pd.DataFrame(df_dict))\n",
    "\n",
    "        df = reduce(\n",
    "            lambda x, y: pd.merge(x, y, on=\"Timestamp\", how=\"outer\"), df_list\n",
    "        )\n",
    "        df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "        all_dfs.append(df)\n",
    "        time.sleep(10)\n",
    "    \n",
    "    for url in bad_urls:\n",
    "        try:\n",
    "            ua = UserAgent()\n",
    "            headers = {\"User-Agent\": ua.random}\n",
    "            response = requests.get(url, headers=headers, timeout=300)\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching URL {url}: {e}\")\n",
    "            continue\n",
    "        startidx = response.text.find(\"(\")\n",
    "        endidx = response.text.find(\")\")\n",
    "\n",
    "        try:\n",
    "            data = loads(response.text[startidx + 1 : endidx])\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSONP from URL {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if \"json\" not in data:\n",
    "            continue\n",
    "\n",
    "        series_list = loads(data[\"json\"])\n",
    "\n",
    "        df_list = []\n",
    "        for series in series_list:\n",
    "            df_dict = {}\n",
    "            region = series[\"label\"]\n",
    "            df_dict[\"Timestamp\"] = [\n",
    "                pd.to_datetime(x[0], unit=\"ms\") for x in series[\"data\"]\n",
    "            ]\n",
    "            df_dict[region] = [int(x[1]) for x in series[\"data\"]]\n",
    "            df_list.append(pd.DataFrame(df_dict))\n",
    "\n",
    "        df = reduce(\n",
    "            lambda x, y: pd.merge(x, y, on=\"Timestamp\", how=\"outer\"), df_list\n",
    "        )\n",
    "        df = df.sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "        all_dfs.append(df)\n",
    "        time.sleep(10)\n",
    "\n",
    "    df_combined = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "    df_combined.to_csv(\"old_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791b035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Central America</th>\n",
       "      <th>Africa</th>\n",
       "      <th>Middle East</th>\n",
       "      <th>Oceania</th>\n",
       "      <th>South America</th>\n",
       "      <th>Russia</th>\n",
       "      <th>Asia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>North America</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-03 10:20:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>142.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-10-03 10:30:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-10-03 10:40:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-03 10:50:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-10-03 11:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>511.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150380</th>\n",
       "      <td>2026-03-01 18:40:00</td>\n",
       "      <td>156</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>3642.0</td>\n",
       "      <td>2933.0</td>\n",
       "      <td>5196.0</td>\n",
       "      <td>11957.0</td>\n",
       "      <td>8303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150381</th>\n",
       "      <td>2026-03-01 18:50:00</td>\n",
       "      <td>161</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>2881.0</td>\n",
       "      <td>4936.0</td>\n",
       "      <td>11918.0</td>\n",
       "      <td>8447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150382</th>\n",
       "      <td>2026-03-01 19:00:00</td>\n",
       "      <td>163</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>2805.0</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>11828.0</td>\n",
       "      <td>8655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150383</th>\n",
       "      <td>2026-03-01 19:10:00</td>\n",
       "      <td>163</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1351.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>3666.0</td>\n",
       "      <td>2721.0</td>\n",
       "      <td>4522.0</td>\n",
       "      <td>11740.0</td>\n",
       "      <td>8833.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150384</th>\n",
       "      <td>2026-03-01 19:20:00</td>\n",
       "      <td>165</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1374.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>3710.0</td>\n",
       "      <td>2661.0</td>\n",
       "      <td>4311.0</td>\n",
       "      <td>11739.0</td>\n",
       "      <td>8921.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150385 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Timestamp  Central America  Africa  Middle East  Oceania  \\\n",
       "0      2016-10-03 10:20:00                2     5.0         29.0     55.0   \n",
       "1      2016-10-03 10:30:00                2     5.0         30.0     53.0   \n",
       "2      2016-10-03 10:40:00                2     5.0         31.0     53.0   \n",
       "3      2016-10-03 10:50:00                2     5.0         31.0     51.0   \n",
       "4      2016-10-03 11:00:00                2     5.0         32.0     51.0   \n",
       "...                    ...              ...     ...          ...      ...   \n",
       "150380 2026-03-01 18:40:00              156   193.0       1373.0    134.0   \n",
       "150381 2026-03-01 18:50:00              161   193.0       1360.0    132.0   \n",
       "150382 2026-03-01 19:00:00              163   183.0       1351.0    130.0   \n",
       "150383 2026-03-01 19:10:00              163   178.0       1351.0    132.0   \n",
       "150384 2026-03-01 19:20:00              165   169.0       1374.0    132.0   \n",
       "\n",
       "        South America  Russia    Asia   Europe  North America  \n",
       "0                31.0   153.0   386.0    465.0          142.0  \n",
       "1                31.0   154.0   391.0    473.0          139.0  \n",
       "2                32.0   154.0   392.0    484.0          134.0  \n",
       "3                32.0   158.0   390.0    498.0          134.0  \n",
       "4                33.0   167.0   402.0    511.0          133.0  \n",
       "...               ...     ...     ...      ...            ...  \n",
       "150380         3642.0  2933.0  5196.0  11957.0         8303.0  \n",
       "150381         3643.0  2881.0  4936.0  11918.0         8447.0  \n",
       "150382         3643.0  2805.0  4721.0  11828.0         8655.0  \n",
       "150383         3666.0  2721.0  4522.0  11740.0         8833.0  \n",
       "150384         3710.0  2661.0  4311.0  11739.0         8921.0  \n",
       "\n",
       "[150385 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_df = pd.read_csv(\"../data/bandwidths.csv\", parse_dates=[\"Timestamp\"])\n",
    "current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c274820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Central America</th>\n",
       "      <th>Africa</th>\n",
       "      <th>Middle East</th>\n",
       "      <th>Oceania</th>\n",
       "      <th>South America</th>\n",
       "      <th>Russia</th>\n",
       "      <th>Asia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>North America</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016-10-29 06:20:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>684</td>\n",
       "      <td>346</td>\n",
       "      <td>717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-10-29 06:30:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>688</td>\n",
       "      <td>375</td>\n",
       "      <td>700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-10-29 06:40:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>688</td>\n",
       "      <td>405</td>\n",
       "      <td>666.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-10-29 06:50:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>682</td>\n",
       "      <td>440</td>\n",
       "      <td>636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-10-29 07:00:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>682</td>\n",
       "      <td>474</td>\n",
       "      <td>617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37357</th>\n",
       "      <td>2024-06-12 16:00:00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>9572</td>\n",
       "      <td>7479</td>\n",
       "      <td>3533.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37358</th>\n",
       "      <td>2024-06-12 16:10:00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>9177</td>\n",
       "      <td>7553</td>\n",
       "      <td>3630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37359</th>\n",
       "      <td>2024-06-12 16:20:00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1096.0</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>8742</td>\n",
       "      <td>7564</td>\n",
       "      <td>3741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37360</th>\n",
       "      <td>2024-06-12 16:30:00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>8579</td>\n",
       "      <td>7676</td>\n",
       "      <td>3879.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37361</th>\n",
       "      <td>2024-06-12 16:40:00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>8036</td>\n",
       "      <td>7785</td>\n",
       "      <td>3932.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3683 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Timestamp  Central America  Africa  Middle East  Oceania  \\\n",
       "11    2016-10-29 06:20:00              5.0     7.0         35.0    107.0   \n",
       "12    2016-10-29 06:30:00              5.0     7.0         36.0    107.0   \n",
       "13    2016-10-29 06:40:00              5.0     8.0         37.0    109.0   \n",
       "14    2016-10-29 06:50:00              5.0     8.0         39.0    107.0   \n",
       "15    2016-10-29 07:00:00              5.0     8.0         40.0    107.0   \n",
       "...                   ...              ...     ...          ...      ...   \n",
       "37357 2024-06-12 16:00:00             55.0   125.0        906.0    216.0   \n",
       "37358 2024-06-12 16:10:00             55.0   122.0        906.0    208.0   \n",
       "37359 2024-06-12 16:20:00             57.0   123.0        911.0    204.0   \n",
       "37360 2024-06-12 16:30:00             57.0   122.0        911.0    194.0   \n",
       "37361 2024-06-12 16:40:00             55.0   122.0        911.0    190.0   \n",
       "\n",
       "       South America  Russia  Asia  Europe  North America  \n",
       "11              84.0   213.0   684     346          717.0  \n",
       "12              82.0   226.0   688     375          700.0  \n",
       "13              80.0   235.0   688     405          666.0  \n",
       "14              77.0   244.0   682     440          636.0  \n",
       "15              74.0   253.0   682     474          617.0  \n",
       "...              ...     ...   ...     ...            ...  \n",
       "37357         1069.0  1991.0  9572    7479         3533.0  \n",
       "37358         1076.0  1985.0  9177    7553         3630.0  \n",
       "37359         1096.0  1974.0  8742    7564         3741.0  \n",
       "37360         1113.0  1958.0  8579    7676         3879.0  \n",
       "37361         1123.0  1959.0  8036    7785         3932.0  \n",
       "\n",
       "[3683 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_df = (\n",
    "    pd.read_csv(\"old_data.csv\", parse_dates=[\"Timestamp\"])\n",
    "    .sort_values(\"Timestamp\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "old_df = old_df.loc[\n",
    "    (old_df[\"Timestamp\"].dt.minute % 10 == 0)\n",
    "    & (old_df[\"Timestamp\"].dt.second == 0)\n",
    "]\n",
    "old_df[\"nan_count\"] = old_df.isnull().sum(axis=1)\n",
    "old_df = (\n",
    "    old_df\n",
    "    .sort_values(by=[\"Timestamp\", \"nan_count\"])\n",
    "    .drop_duplicates(subset=\"Timestamp\", keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "old_df = old_df.drop(columns=[\"nan_count\"])\n",
    "\n",
    "# Filter to values not in current_df\n",
    "old_df = old_df.loc[~old_df[\"Timestamp\"].isin(current_df[\"Timestamp\"])]\n",
    "old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6291e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([old_df, current_df], axis=0, ignore_index=True).sort_values(\"Timestamp\").reset_index(drop=True)\n",
    "regions_all = [\n",
    "    \"Central America\",\n",
    "    \"Africa\",\n",
    "    \"Middle East\",\n",
    "    \"Oceania\",\n",
    "    \"South America\",\n",
    "    \"Russia\",\n",
    "    \"Asia\",\n",
    "    \"Europe\",\n",
    "    \"North America\",\n",
    "]\n",
    "combined[regions_all] = combined[regions_all].astype(\"Int64\")\n",
    "combined.to_csv(\"../data/bandwidths.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Steam-Time-Series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
